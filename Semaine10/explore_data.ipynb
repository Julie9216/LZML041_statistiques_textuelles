{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1jk8ild</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Macron propose d'expulser les locataires des H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-26 11:22:51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1j6cwa4</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Ukraine - Russie : Macron fait n'importe quoi !</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-08 09:34:16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iuxpxt</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Le pacte secret entre Macron et Le Pen ! ‚Äì Pan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-21 19:20:55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1it9v25</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Marine Le Pen a soutenu un ami de Macron Ôøº</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-19 17:39:24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1imtylh</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Fichage, surveillance, tra√ßage : le gouvernent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-11 09:22:52</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   author                                              title Text  \\\n",
       "0  1jk8ild  Nohan07  Macron propose d'expulser les locataires des H...  NaN   \n",
       "1  1j6cwa4  Nohan07    Ukraine - Russie : Macron fait n'importe quoi !  NaN   \n",
       "2  1iuxpxt  Nohan07  Le pacte secret entre Macron et Le Pen ! ‚Äì Pan...  NaN   \n",
       "3  1it9v25  Nohan07         Marine Le Pen a soutenu un ami de Macron Ôøº  NaN   \n",
       "4  1imtylh  Nohan07  Fichage, surveillance, tra√ßage : le gouvernent...  NaN   \n",
       "\n",
       "         subreddit              created  score  \n",
       "0  FranceInsoumise  2025-03-26 11:22:51      2  \n",
       "1  FranceInsoumise  2025-03-08 09:34:16      2  \n",
       "2  FranceInsoumise  2025-02-21 19:20:55      3  \n",
       "3  FranceInsoumise  2025-02-19 17:39:24      5  \n",
       "4  FranceInsoumise  2025-02-11 09:22:52      5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Reddit_macron.csv\", encoding='utf-8') # ouvrir le fichier csv\n",
    "\n",
    "#Cr√©er un copie de Travail en choisissant uniquement les colonnes qui nous int√©ressent\n",
    "df_nd = df[['id','author','title','Text','subreddit','created','score']].copy()\n",
    "df_nd = df_nd.drop_duplicates().copy()\n",
    "df_nd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1jk8ild</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Macron propose d'expulser les locataires des H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-26 11:22:51</td>\n",
       "      <td>2</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1j6cwa4</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Ukraine - Russie : Macron fait n'importe quoi !</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-08 09:34:16</td>\n",
       "      <td>2</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iuxpxt</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Le pacte secret entre Macron et Le Pen ! ‚Äì Pan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-21 19:20:55</td>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1it9v25</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Marine Le Pen a soutenu un ami de Macron Ôøº</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-19 17:39:24</td>\n",
       "      <td>5</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1imtylh</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Fichage, surveillance, tra√ßage : le gouvernent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-11 09:22:52</td>\n",
       "      <td>5</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb-2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   author                                              title Text  \\\n",
       "0  1jk8ild  Nohan07  Macron propose d'expulser les locataires des H...  NaN   \n",
       "1  1j6cwa4  Nohan07    Ukraine - Russie : Macron fait n'importe quoi !  NaN   \n",
       "2  1iuxpxt  Nohan07  Le pacte secret entre Macron et Le Pen ! ‚Äì Pan...  NaN   \n",
       "3  1it9v25  Nohan07         Marine Le Pen a soutenu un ami de Macron Ôøº  NaN   \n",
       "4  1imtylh  Nohan07  Fichage, surveillance, tra√ßage : le gouvernent...  NaN   \n",
       "\n",
       "         subreddit              created  score  year  month month_name  \\\n",
       "0  FranceInsoumise  2025-03-26 11:22:51      2  2025      3        Mar   \n",
       "1  FranceInsoumise  2025-03-08 09:34:16      2  2025      3        Mar   \n",
       "2  FranceInsoumise  2025-02-21 19:20:55      3  2025      2        Feb   \n",
       "3  FranceInsoumise  2025-02-19 17:39:24      5  2025      2        Feb   \n",
       "4  FranceInsoumise  2025-02-11 09:22:52      5  2025      2        Feb   \n",
       "\n",
       "     period  \n",
       "0  Mar-2025  \n",
       "1  Mar-2025  \n",
       "2  Feb-2025  \n",
       "3  Feb-2025  \n",
       "4  Feb-2025  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import calendar\n",
    "\n",
    "df_nd.loc[:, 'year'] = pd.to_datetime(df_nd.created, format='%Y-%m-%d %H:%M:%S').dt.year\n",
    "df_nd.loc[:, 'month'] = pd.to_datetime(df_nd.created, format='%Y-%m-%d %H:%M:%S').dt.month\n",
    "df_nd['month_name'] = df_nd[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "cols=[\"month_name\",\"year\"]\n",
    "df_nd['period'] = df_nd[cols].apply(lambda x: '-'.join(x.values.astype(str)), axis=\"columns\")\n",
    "df_nd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat_year=[]\n",
    "for i in df_nd.index:\n",
    "    if df_nd['month'][i] >= 5:\n",
    "        mandat_year.append(df_nd['year'][i]-2017)\n",
    "    else:\n",
    "        mandat_year.append(df_nd['year'][i]-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119</td>\n",
       "      <td>15</td>\n",
       "      <td>119</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>119</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>251</td>\n",
       "      <td>24</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>433</td>\n",
       "      <td>32</td>\n",
       "      <td>422</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>433</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201</td>\n",
       "      <td>34</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>201</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  author  title  Text  subreddit  created  score  year  month  \\\n",
       "mandat                                                                     \n",
       "4       119      15    119    11          3      119     24     2      8   \n",
       "5       251      24    249     1          5      250     26     2     12   \n",
       "6       433      32    422     2          6      433     31     2     12   \n",
       "7       201      34    197     0          6      201     61     2     12   \n",
       "\n",
       "        month_name  period  \n",
       "mandat                      \n",
       "4                8       8  \n",
       "5               12      12  \n",
       "6               12      12  \n",
       "7               12      12  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nd['mandat'] = mandat_year\n",
    "df_nd.groupby('mandat').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>period</th>\n",
       "      <th>mandat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1847pf8</td>\n",
       "      <td>konbini</td>\n",
       "      <td>G√©rard Collomb, ancien maire de Lyon et ancien...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2023-11-26 11:16:42</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Nov</td>\n",
       "      <td>Nov-2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>154qm6b</td>\n",
       "      <td>konbini</td>\n",
       "      <td>Alors, √ßa dit quoi ce remaniement d‚ÄôEmmanuel M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2023-07-20 15:14:05</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>Jul</td>\n",
       "      <td>Jul-2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>123sc94</td>\n",
       "      <td>DrogDrill</td>\n",
       "      <td>Il faut faire tomber le gouvernement Macron!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2023-03-27 18:31:34</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar-2023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>yn0om1</td>\n",
       "      <td>infofrance</td>\n",
       "      <td>Emmanuel Macron ne sera pas au d√©part de la Ro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2022-11-05 18:36:54</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>Nov</td>\n",
       "      <td>Nov-2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>y72b5s</td>\n",
       "      <td>konbini</td>\n",
       "      <td>Macron f√©licite Benzema sur Twitter, la France...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2022-10-18 11:38:11</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>Oct</td>\n",
       "      <td>Oct-2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      author                                              title  \\\n",
       "999   1847pf8     konbini  G√©rard Collomb, ancien maire de Lyon et ancien...   \n",
       "1000  154qm6b     konbini  Alors, √ßa dit quoi ce remaniement d‚ÄôEmmanuel M...   \n",
       "1001  123sc94   DrogDrill       Il faut faire tomber le gouvernement Macron!   \n",
       "1002   yn0om1  infofrance  Emmanuel Macron ne sera pas au d√©part de la Ro...   \n",
       "1003   y72b5s     konbini  Macron f√©licite Benzema sur Twitter, la France...   \n",
       "\n",
       "     Text   subreddit              created  score  year  month month_name  \\\n",
       "999   NaN  franceinfo  2023-11-26 11:16:42      0  2023     11        Nov   \n",
       "1000  NaN  franceinfo  2023-07-20 15:14:05      0  2023      7        Jul   \n",
       "1001  NaN  franceinfo  2023-03-27 18:31:34      1  2023      3        Mar   \n",
       "1002  NaN  franceinfo  2022-11-05 18:36:54      1  2022     11        Nov   \n",
       "1003  NaN  franceinfo  2022-10-18 11:38:11      0  2022     10        Oct   \n",
       "\n",
       "        period  mandat  \n",
       "999   Nov-2023       6  \n",
       "1000  Jul-2023       6  \n",
       "1001  Mar-2023       5  \n",
       "1002  Nov-2022       5  \n",
       "1003  Oct-2022       5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macron propose d'expulser les locataires des HLM et tente de les faire passer pour des privil√©gi√©s.\n"
     ]
    }
   ],
   "source": [
    "list_period=df_nd[\"period\"].drop_duplicates().tolist()\n",
    "list_period\n",
    "\n",
    "list_posts =df_nd['title'].tolist()\n",
    "# cr√©er une liste regroupant tous les posts par mois \n",
    "Collection = [ ]\n",
    "for p in list_period:\n",
    "    Collection.append(df_nd['title'][df_nd[\"period\"] == p].tolist())\n",
    "    \n",
    "len(Collection)\n",
    "print(Collection[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Corpus\n",
    "Les pr√©traitement de votre corpus va d√©pendre en grande partie √† la qualit√© de vos donn√©es.\n",
    "Il faut toutefois garder √† l‚Äôesprit que certains des pr√©traitement ne sont pas n√©cessaires selon le mod√®le de machine learning que vous utiliserez par la suite ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_english  = set(stopwords.words('english'))\n",
    "stop_words_french   = set(stopwords.words('french'))\n",
    "stop_words_specific = [\"'\", \"le\",\"ce\",\"de\",\"cest\",\"celui\",\"ne\"]\n",
    "stop_words          = set(stopwords.words('french') + stopwords.words('english') + stop_words_specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √©tape 1 : Basic Cleaning - √† adapter √† votre corpus/imagination !\n",
    "\n",
    "- Remove Unicode Strings and Noise\n",
    "- Remove/Replace URLs, User Mentions and Hashtags\n",
    "- Non-Letter characters: numbers, emojis, or hash marks.\n",
    "- Remove/Replace Slang and Abbreviations\n",
    "- Remove/Replace Contractions\n",
    "- Remove/Replace Numbers\n",
    "- Remove/Replace Repetitions of Punctuation\n",
    "- Remove Punctuation\n",
    "- Handling Capitalized Words / Lowercase\n",
    "- Replace Elongated Words (ex: hahahaaaa, ‚ÄòDuuuuude, that's awful,‚Äô‚Äù)\n",
    "\n",
    "https://pynative.com/python-regex-replace-re-sub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following pre-tokenization receives string as input parameter\n",
    "#and returns string as output\n",
    "import re\n",
    "#import contractions\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet) # remove Twitter links\n",
    "    return tweet\n",
    "\n",
    "def remove_tags(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    tweet = re.sub('RT @[\\w_]+:','', tweet)  # remove retweet label\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "punctuation = '!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@``'\n",
    "def remove_nonText(tweet):\n",
    "    tweet = re.sub('[' + punctuation + ']+', '', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub(r'\\n','', tweet)  # remove escape sequence\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet) # un exemple d'image que vous pouvez compl√©ter !\n",
    "\n",
    "    return tweet\n",
    "\n",
    "#def remove_contraction(text):\n",
    "  #  return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "def pretokenization_cleaning(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_tags(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_nonText(tweet)\n",
    "#    tweet = remove_contraction(tweet) # English only !\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Macron propose d'expulser les locataires des HLM et tente de les faire passer pour des privil√©gi√©s.\", \"Ukraine - Russie : Macron fait n'importe quoi !\", 'Allocution du Pr√©sident de la R√©publique Emmanuel Macron - 05/03/2025', \"Emmanuel Macron a-t-il eu une influence sur l'arriv√©e de soldats russes en Centrafrique ?\", 'Charles de Courson : Macron cauchemarde, la France r√©siste ? [EN DIRECT]', '\"Il veut nous faire peur\" : la classe politique r√©agit √† l\\'allocution d\\'Emmanuel Macron : Actualit√©s - Orange', 'Qui est Candace Owens, l‚Äôinfluenceuse trumpiste qui relaie l‚Äôinfox transphobe sur Brigitte Macron ?', 'Face √† la ‚Äúmenace russe‚Äù, Macron propose d‚Äôouvrir le d√©bat sur un parapluie nucl√©aire europ√©en', \"Dissuasion nucl√©aire: les propos d'Emmanuel Macron braquent les autorit√©s russes\", 'Blanchiment d‚Äôargent russe : des perquisitions men√©es chez l‚Äôassoci√© de Tiphaine Auzi√®re, fille de Brigitte Macron']\n"
     ]
    }
   ],
   "source": [
    "Text=Collection[0][0:10]\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librarie Spacy \n",
    "SpaCy est une biblioth√®que Python open source pour le traitement du texte et des langues naturelles. Cr√©√© par l‚Äô√©quipe de la startup Explosion AI, spaCy a √©t√© publi√©e sous la licence MIT en 2015 et ses principaux d√©veloppeurs sont Matthew Honnibal et Ines Montani, les fondateurs de la startup.\n",
    "- Contrairement √† NLTK, qui est largement utilis√© pour l‚Äôenseignement et la recherche, spaCy est con√ßu pour l‚Äôutilisation de production sur de grandes quantit√©s de textes avec une excellente performance et pr√©cision\n",
    "- Il prend en charge des mod√®les statistiques pour 21 langues dont le fran√ßais, l‚Äôanglais, l‚Äôallemand, l‚Äôespagnol, l‚Äôitalien, le portugais et le n√©erlandais. \n",
    "- Vous pouvez √©couter le tuto de Cl√©ment Plancq ! https://github.com/clement-plancq/tuto-mate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installer Spacy dans terminal\n",
    "#conda install -c conda-forge spacy\n",
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download fr_core_news_md\n",
    "#!python -m spacy download fr_core_news_md\n",
    "#conda install -c conda-forge spacy-model-fr_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √©tape 2 : Normalising data  - √† adapter √† votre corpus \n",
    "- Spelling Correction\n",
    "- Replace Negations with Antonyms\n",
    "- Handling Capitalized Words\n",
    "- Lowercase\n",
    "- Tokenization\n",
    "- Remove Stopwords (ex: the, and‚Ä¶.)\n",
    "- Stemming / racinisation (optional)\n",
    "- Lemmatizing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowball\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sentences \n\u001b[0;32m      5\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "stemmer = SnowballStemmer(language='french')\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "#    return \" \".join([token for token in text if token.lower() not in stop_words])\n",
    "    return [token for token in text if token.lower() not in stop_words_french]\n",
    "\n",
    "def stemming(text):\n",
    "    return [stemmer.stem(token) for token in text]\n",
    "\n",
    "def lemmatizing(text):\n",
    "    Text = \" \".join([token for token in text])\n",
    "    doc = nlp(Text)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizing(remove_stopwords(tokenize(Text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add stopwors to your list \n",
    "#stop_words_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_post(post):\n",
    "    \"\"\"Main master function to clean and normalizing posts, and tokenizing / Lemmatization\"\"\"\n",
    "    tweet = remove_links(post)\n",
    "    tweet = remove_tags(post)\n",
    "    tweet = remove_users(post)\n",
    "    tweet = remove_nonText(post)\n",
    " #   tweet = remove_contraction(tweet) # English only\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = tokenize(post)  # apply tokenization\n",
    "    tweet = remove_stopwords(post)\n",
    "#    tweet = stemming(tweet) # Optionnel \n",
    "    tweet = lemmatizing(tweet) # Optionnel\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr√©er une liste regroupant tous les posts\n",
    "list_posts_clean = [ ]\n",
    "for post in list_posts:\n",
    "    list_posts_clean.append(tokenize(preprocess_post(post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_posts_clean[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "#Create a list of all words\n",
    "all_tokens_clean = sum(list_posts_clean, [])\n",
    "fdist_normalized = FreqDist(all_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Top 50 in all Macron posts : ---\" , fdist_normalized.most_common(50))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr√©er une liste regroupant tous les textes par mois\n",
    "Collection_clean = [ ]\n",
    "\n",
    "for i in range(len(Collection)):\n",
    "    Collection_clean.append([tokenize(preprocess_post(text)) for text in Collection[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in Collection_clean[0][0:10]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "for i in range(len(Collection_clean)):\n",
    "    all_tokens_clean_temp = sum(Collection_clean[i], [])\n",
    "    fdist_normalized_temp = FreqDist(all_tokens_clean_temp)\n",
    "    print( \"mois de\", list_period[i] ,\" : ---\" , )\n",
    "    print(fdist_normalized_temp.most_common(10))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche contextuelle : les n-gramms\n",
    "L‚Äôapproche contextuelle : on s‚Äôint√©resse non seulement aux mots et √† leur fr√©quence, mais aussi aux mots qui suivent.\n",
    "Le calcul de n-grams (bigrams pour les co-occurences de mots deux-√†-deux, tri-grams pour les co-occurences trois-√†-trois, etc.) constitue la m√©thode la plus simple pour tenir compte du contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - Co-occurences\n",
    "Une cooccurrence est une combinaison de mots qui apparaissent fr√©quemment ensemble dans un corpus. Ainsi on retrouve dans les m√™me textes les termes : ¬´ professeurs, √©tudiants ¬ª, ¬´ m√©decin, infirmi√®re ¬ª, ou encore ¬´ pain, boulangerie ¬ª...\n",
    "Les collocations sont eux une forme privil√©gi√©e de cooccurrence : ce sont tout simplement des mots qui tendent √† appara√Ætre ensemble. Parmi les collocations en langues fran√ßaise, nous pouvons citer : ¬´ un d√©sir ardent ¬ª, ¬´ un l√©ger frisson ¬ª, ou encore ¬´ un regret am√®re ¬ª. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_tokens_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mText(\u001b[43mall_tokens_clean\u001b[49m)\n\u001b[0;32m      2\u001b[0m text\u001b[38;5;241m.\u001b[39mconcordance(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretraites\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_tokens_clean' is not defined"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(all_tokens_clean)\n",
    "text.concordance(\"retraites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent collocations in the text (usually meaningful phrases)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Ngrams with the specific name \"guerre\"\n",
    "name_filter = lambda *w: 'guerre' not in w\n",
    "\n",
    "## Bigrams\n",
    "finder = nltk.BigramCollocationFinder.from_words(text)\n",
    "\n",
    "# only bigrams that contain 'guerre'\n",
    "finder.apply_ngram_filter(name_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest student ratio\n",
    "print(finder.nbest(bigram_measures.student_t,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest chi2 ratio\n",
    "print(finder.nbest(bigram_measures.chi_sq,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest likelihood_ratio\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest PMI ratio\n",
    "print(finder.nbest(bigram_measures.pmi,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer une liste pour afficher les r√©sultats en tableau\n",
    "list_bigrams=[]\n",
    "list_bigrams.append(finder.nbest(bigram_measures.student_t,10))\n",
    "list_bigrams.append(finder.nbest(bigram_measures.chi_sq,10))\n",
    "list_bigrams.append(finder.nbest(bigram_measures.likelihood_ratio,10))\n",
    "list_bigrams.append(finder.nbest(bigram_measures.pmi,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_bigrams)\n",
    "df=df.transpose()\n",
    "df.columns = ['Student_t', 'Chi_sq', 'Likelihood_ratio','PMI']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Explore Co-occurring Words (Bigrams)\n",
    "Une cooccurrence est une combinaison de mots qui apparaissent fr√©quemment ensemble dans un corpus. Ainsi on retrouve dans les m√™me textes les termes : ¬´ professeurs, √©tudiants ¬ª, ¬´ m√©decin, infirmi√®re ¬ª, ou encore ¬´ pain, boulangerie ¬ª...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram = list(bigrams(all_tokens_clean))\n",
    "# View bigrams \n",
    "terms_bigram[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(terms_bigram)\n",
    "\n",
    "bigram_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(100),\n",
    "                             columns=['bigram', 'count'])\n",
    "\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c- Visualize Networks of Bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of bigrams and their counts\n",
    "d = bigram_df.set_index('bigram').T.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network plot \n",
    "G = nx.Graph()\n",
    "\n",
    "# Create connections between nodes\n",
    "for k, v in d[0].items():\n",
    "    G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "\n",
    "G.add_node(\"macron\", weight=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "pos = nx.spring_layout(G, k=2)\n",
    "\n",
    "# Plot networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=12,\n",
    "                 width=2,\n",
    "                 edge_color='grey',\n",
    "                 node_color='blue',\n",
    "                 node_size=50,\n",
    "                 with_labels = False,\n",
    "                 ax=ax)\n",
    "\n",
    "# Create offset labels\n",
    "for key, value in pos.items():\n",
    "    x, y = value[0]+.135, value[1]+.145\n",
    "    ax.text(x, y,\n",
    "            s=key,\n",
    "            bbox=dict(facecolor='lightblue', alpha=0.25),\n",
    "            horizontalalignment='center', fontsize=8)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - The Similarity Between Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a - Euclidean Distance\n",
    "Euclidean Distance is probably one of the most known formulas for computing the distance between two points applying the Pythagorean theorem. To get it you just need to subtract the points from the vectors, raise them to squares, add them up and take the square root of them. Did it seem complex? Don‚Äôt worry, in the image below it will be easier to understand.\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "df_similarity_euclidean = pd.DataFrame(euclidean_distances(vectors, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_euclidean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat = 2\n",
    "Similar_years=pd.DataFrame()\n",
    "print(\"We look for the most similar posts to: \")\n",
    "Similar_years ['Euclidean similarity'] = pd.DataFrame(df_similarity_euclidean[mandat])\n",
    "Similar_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similar_years.sort_values(by=['Euclidean similarity'] ,ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b - Manhattan Distance\n",
    "Although Manhattan distance seems to work okay for high-dimensional data, it is a measure that is somewhat less intuitive than euclidean distance, especially when using in high-dimensional data.\n",
    "\n",
    "When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "df_similarity_manhattan = pd.DataFrame(manhattan_distances(vectors, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_manhattan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat = 2\n",
    "print(\"We look for the most similar tweets to: \")\n",
    "Similar_years ['Manhattan similarity'] = pd.DataFrame(df_similarity_manhattan[mandat])\n",
    "Similar_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similar_years.sort_values(by=['Manhattan similarity'] ,ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c - Finding similar documents using cosine similarity\n",
    "Cosine Similarity is a method of calculating the similarity of two vectors by taking the dot product and dividing it by the magnitudes of each vector, as shown by the illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "url = \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*IhpY-6LYV75983THCpWo-w.png\"\n",
    "Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df_similarity_cosine = pd.DataFrame(cosine_similarity(vectors, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat = 2\n",
    "print(\"We look for the most yers to: \" )\n",
    "Similar_years ['Cosine similarity'] = pd.DataFrame(df_similarity_cosine[mandat])\n",
    "Similar_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similar_years.sort_values(by=['Cosine similarity'] ,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes\n",
    "fig = plt.figure(figsize=(17,10), dpi=70)\n",
    "ax = fig.add_subplot(111)\n",
    "# Visualize the matrix with colored squares indicating similarity\n",
    "ax.matshow(df_similarity_cosine, cmap='rainbow_r', vmin = 0.0, vmax = 1)\n",
    "# Set regular ticks, one for each book in the collection\n",
    "ax.set_xticks(range(len(df_results[\"period\"])))\n",
    "ax.set_yticks(range(len(df_results[\"period\"])))\n",
    "# Set the tick labels as the book titles\n",
    "ax.set_xticklabels(df_results[\"period\"])\n",
    "ax.set_yticklabels(df_results[\"period\"])\n",
    "# Rotate the labels on the x-axis by 90 degrees\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
