{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1jk8ild</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Macron propose d'expulser les locataires des H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-26 11:22:51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1j6cwa4</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Ukraine - Russie : Macron fait n'importe quoi !</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-08 09:34:16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iuxpxt</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Le pacte secret entre Macron et Le Pen ! – Pan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-21 19:20:55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1it9v25</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Marine Le Pen a soutenu un ami de Macron ￼</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-19 17:39:24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1imtylh</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Fichage, surveillance, traçage : le gouvernent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-11 09:22:52</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   author                                              title Text  \\\n",
       "0  1jk8ild  Nohan07  Macron propose d'expulser les locataires des H...  NaN   \n",
       "1  1j6cwa4  Nohan07    Ukraine - Russie : Macron fait n'importe quoi !  NaN   \n",
       "2  1iuxpxt  Nohan07  Le pacte secret entre Macron et Le Pen ! – Pan...  NaN   \n",
       "3  1it9v25  Nohan07         Marine Le Pen a soutenu un ami de Macron ￼  NaN   \n",
       "4  1imtylh  Nohan07  Fichage, surveillance, traçage : le gouvernent...  NaN   \n",
       "\n",
       "         subreddit              created  score  \n",
       "0  FranceInsoumise  2025-03-26 11:22:51      2  \n",
       "1  FranceInsoumise  2025-03-08 09:34:16      2  \n",
       "2  FranceInsoumise  2025-02-21 19:20:55      3  \n",
       "3  FranceInsoumise  2025-02-19 17:39:24      5  \n",
       "4  FranceInsoumise  2025-02-11 09:22:52      5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Reddit_macron.csv\", encoding='utf-8') # ouvrir le fichier csv\n",
    "\n",
    "#Créer un copie de Travail en choisissant uniquement les colonnes qui nous intéressent\n",
    "df_nd = df[['id','author','title','Text','subreddit','created','score']].copy()\n",
    "df_nd = df_nd.drop_duplicates().copy()\n",
    "df_nd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1jk8ild</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Macron propose d'expulser les locataires des H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-26 11:22:51</td>\n",
       "      <td>2</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1j6cwa4</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Ukraine - Russie : Macron fait n'importe quoi !</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-03-08 09:34:16</td>\n",
       "      <td>2</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iuxpxt</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Le pacte secret entre Macron et Le Pen ! – Pan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-21 19:20:55</td>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1it9v25</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Marine Le Pen a soutenu un ami de Macron ￼</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-19 17:39:24</td>\n",
       "      <td>5</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1imtylh</td>\n",
       "      <td>Nohan07</td>\n",
       "      <td>Fichage, surveillance, traçage : le gouvernent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FranceInsoumise</td>\n",
       "      <td>2025-02-11 09:22:52</td>\n",
       "      <td>5</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb-2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   author                                              title Text  \\\n",
       "0  1jk8ild  Nohan07  Macron propose d'expulser les locataires des H...  NaN   \n",
       "1  1j6cwa4  Nohan07    Ukraine - Russie : Macron fait n'importe quoi !  NaN   \n",
       "2  1iuxpxt  Nohan07  Le pacte secret entre Macron et Le Pen ! – Pan...  NaN   \n",
       "3  1it9v25  Nohan07         Marine Le Pen a soutenu un ami de Macron ￼  NaN   \n",
       "4  1imtylh  Nohan07  Fichage, surveillance, traçage : le gouvernent...  NaN   \n",
       "\n",
       "         subreddit              created  score  year  month month_name  \\\n",
       "0  FranceInsoumise  2025-03-26 11:22:51      2  2025      3        Mar   \n",
       "1  FranceInsoumise  2025-03-08 09:34:16      2  2025      3        Mar   \n",
       "2  FranceInsoumise  2025-02-21 19:20:55      3  2025      2        Feb   \n",
       "3  FranceInsoumise  2025-02-19 17:39:24      5  2025      2        Feb   \n",
       "4  FranceInsoumise  2025-02-11 09:22:52      5  2025      2        Feb   \n",
       "\n",
       "     period  \n",
       "0  Mar-2025  \n",
       "1  Mar-2025  \n",
       "2  Feb-2025  \n",
       "3  Feb-2025  \n",
       "4  Feb-2025  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import calendar\n",
    "\n",
    "df_nd.loc[:, 'year'] = pd.to_datetime(df_nd.created, format='%Y-%m-%d %H:%M:%S').dt.year\n",
    "df_nd.loc[:, 'month'] = pd.to_datetime(df_nd.created, format='%Y-%m-%d %H:%M:%S').dt.month\n",
    "df_nd['month_name'] = df_nd[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "cols=[\"month_name\",\"year\"]\n",
    "df_nd['period'] = df_nd[cols].apply(lambda x: '-'.join(x.values.astype(str)), axis=\"columns\")\n",
    "df_nd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat_year=[]\n",
    "for i in df_nd.index:\n",
    "    if df_nd['month'][i] >= 5:\n",
    "        mandat_year.append(df_nd['year'][i]-2017)\n",
    "    else:\n",
    "        mandat_year.append(df_nd['year'][i]-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119</td>\n",
       "      <td>15</td>\n",
       "      <td>119</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>119</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>251</td>\n",
       "      <td>24</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>433</td>\n",
       "      <td>32</td>\n",
       "      <td>422</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>433</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201</td>\n",
       "      <td>34</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>201</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  author  title  Text  subreddit  created  score  year  month  \\\n",
       "mandat                                                                     \n",
       "4       119      15    119    11          3      119     24     2      8   \n",
       "5       251      24    249     1          5      250     26     2     12   \n",
       "6       433      32    422     2          6      433     31     2     12   \n",
       "7       201      34    197     0          6      201     61     2     12   \n",
       "\n",
       "        month_name  period  \n",
       "mandat                      \n",
       "4                8       8  \n",
       "5               12      12  \n",
       "6               12      12  \n",
       "7               12      12  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nd['mandat'] = mandat_year\n",
    "df_nd.groupby('mandat').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_name</th>\n",
       "      <th>period</th>\n",
       "      <th>mandat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1847pf8</td>\n",
       "      <td>konbini</td>\n",
       "      <td>Gérard Collomb, ancien maire de Lyon et ancien...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2023-11-26 11:16:42</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Nov</td>\n",
       "      <td>Nov-2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>154qm6b</td>\n",
       "      <td>konbini</td>\n",
       "      <td>Alors, ça dit quoi ce remaniement d’Emmanuel M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2023-07-20 15:14:05</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>Jul</td>\n",
       "      <td>Jul-2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>123sc94</td>\n",
       "      <td>DrogDrill</td>\n",
       "      <td>Il faut faire tomber le gouvernement Macron!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2023-03-27 18:31:34</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar-2023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>yn0om1</td>\n",
       "      <td>infofrance</td>\n",
       "      <td>Emmanuel Macron ne sera pas au départ de la Ro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2022-11-05 18:36:54</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>Nov</td>\n",
       "      <td>Nov-2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>y72b5s</td>\n",
       "      <td>konbini</td>\n",
       "      <td>Macron félicite Benzema sur Twitter, la France...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>franceinfo</td>\n",
       "      <td>2022-10-18 11:38:11</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>Oct</td>\n",
       "      <td>Oct-2022</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      author                                              title  \\\n",
       "999   1847pf8     konbini  Gérard Collomb, ancien maire de Lyon et ancien...   \n",
       "1000  154qm6b     konbini  Alors, ça dit quoi ce remaniement d’Emmanuel M...   \n",
       "1001  123sc94   DrogDrill       Il faut faire tomber le gouvernement Macron!   \n",
       "1002   yn0om1  infofrance  Emmanuel Macron ne sera pas au départ de la Ro...   \n",
       "1003   y72b5s     konbini  Macron félicite Benzema sur Twitter, la France...   \n",
       "\n",
       "     Text   subreddit              created  score  year  month month_name  \\\n",
       "999   NaN  franceinfo  2023-11-26 11:16:42      0  2023     11        Nov   \n",
       "1000  NaN  franceinfo  2023-07-20 15:14:05      0  2023      7        Jul   \n",
       "1001  NaN  franceinfo  2023-03-27 18:31:34      1  2023      3        Mar   \n",
       "1002  NaN  franceinfo  2022-11-05 18:36:54      1  2022     11        Nov   \n",
       "1003  NaN  franceinfo  2022-10-18 11:38:11      0  2022     10        Oct   \n",
       "\n",
       "        period  mandat  \n",
       "999   Nov-2023       6  \n",
       "1000  Jul-2023       6  \n",
       "1001  Mar-2023       5  \n",
       "1002  Nov-2022       5  \n",
       "1003  Oct-2022       5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macron propose d'expulser les locataires des HLM et tente de les faire passer pour des privilégiés.\n"
     ]
    }
   ],
   "source": [
    "list_period=df_nd[\"period\"].drop_duplicates().tolist()\n",
    "list_period\n",
    "\n",
    "list_posts =df_nd['title'].tolist()\n",
    "# créer une liste regroupant tous les posts par mois \n",
    "Collection = [ ]\n",
    "for p in list_period:\n",
    "    Collection.append(df_nd['title'][df_nd[\"period\"] == p].tolist())\n",
    "    \n",
    "len(Collection)\n",
    "print(Collection[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Corpus\n",
    "Les prétraitement de votre corpus va dépendre en grande partie à la qualité de vos données.\n",
    "Il faut toutefois garder à l’esprit que certains des prétraitement ne sont pas nécessaires selon le modèle de machine learning que vous utiliserez par la suite ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_english  = set(stopwords.words('english'))\n",
    "stop_words_french   = set(stopwords.words('french'))\n",
    "stop_words_specific = [\"'\", \"le\",\"ce\",\"de\",\"cest\",\"celui\",\"ne\"]\n",
    "stop_words          = set(stopwords.words('french') + stopwords.words('english') + stop_words_specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## étape 1 : Basic Cleaning - à adapter à votre corpus/imagination !\n",
    "\n",
    "- Remove Unicode Strings and Noise\n",
    "- Remove/Replace URLs, User Mentions and Hashtags\n",
    "- Non-Letter characters: numbers, emojis, or hash marks.\n",
    "- Remove/Replace Slang and Abbreviations\n",
    "- Remove/Replace Contractions\n",
    "- Remove/Replace Numbers\n",
    "- Remove/Replace Repetitions of Punctuation\n",
    "- Remove Punctuation\n",
    "- Handling Capitalized Words / Lowercase\n",
    "- Replace Elongated Words (ex: hahahaaaa, ‘Duuuuude, that's awful,’”)\n",
    "\n",
    "https://pynative.com/python-regex-replace-re-sub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following pre-tokenization receives string as input parameter\n",
    "#and returns string as output\n",
    "import re\n",
    "#import contractions\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet) # remove Twitter links\n",
    "    return tweet\n",
    "\n",
    "def remove_tags(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    tweet = re.sub('RT @[\\w_]+:','', tweet)  # remove retweet label\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "punctuation = '!”$%&\\’()*+,-./:;<=>?[\\\\]^_`{|}~•@``'\n",
    "def remove_nonText(tweet):\n",
    "    tweet = re.sub('[' + punctuation + ']+', '', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub(r'\\n','', tweet)  # remove escape sequence\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('📝 …', '', tweet) # un exemple d'image que vous pouvez compléter !\n",
    "\n",
    "    return tweet\n",
    "\n",
    "#def remove_contraction(text):\n",
    "  #  return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "def pretokenization_cleaning(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_tags(tweet)\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_nonText(tweet)\n",
    "#    tweet = remove_contraction(tweet) # English only !\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Macron propose d'expulser les locataires des HLM et tente de les faire passer pour des privilégiés.\", \"Ukraine - Russie : Macron fait n'importe quoi !\", 'Allocution du Président de la République Emmanuel Macron - 05/03/2025', \"Emmanuel Macron a-t-il eu une influence sur l'arrivée de soldats russes en Centrafrique ?\", 'Charles de Courson : Macron cauchemarde, la France résiste ? [EN DIRECT]', '\"Il veut nous faire peur\" : la classe politique réagit à l\\'allocution d\\'Emmanuel Macron : Actualités - Orange', 'Qui est Candace Owens, l’influenceuse trumpiste qui relaie l’infox transphobe sur Brigitte Macron ?', 'Face à la “menace russe”, Macron propose d’ouvrir le débat sur un parapluie nucléaire européen', \"Dissuasion nucléaire: les propos d'Emmanuel Macron braquent les autorités russes\", 'Blanchiment d’argent russe : des perquisitions menées chez l’associé de Tiphaine Auzière, fille de Brigitte Macron']\n"
     ]
    }
   ],
   "source": [
    "Text=Collection[0][0:10]\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librarie Spacy \n",
    "SpaCy est une bibliothèque Python open source pour le traitement du texte et des langues naturelles. Créé par l’équipe de la startup Explosion AI, spaCy a été publiée sous la licence MIT en 2015 et ses principaux développeurs sont Matthew Honnibal et Ines Montani, les fondateurs de la startup.\n",
    "- Contrairement à NLTK, qui est largement utilisé pour l’enseignement et la recherche, spaCy est conçu pour l’utilisation de production sur de grandes quantités de textes avec une excellente performance et précision\n",
    "- Il prend en charge des modèles statistiques pour 21 langues dont le français, l’anglais, l’allemand, l’espagnol, l’italien, le portugais et le néerlandais. \n",
    "- Vous pouvez écouter le tuto de Clément Plancq ! https://github.com/clement-plancq/tuto-mate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installer Spacy dans terminal\n",
    "#conda install -c conda-forge spacy\n",
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download fr_core_news_md\n",
    "#!python -m spacy download fr_core_news_md\n",
    "#conda install -c conda-forge spacy-model-fr_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## étape 2 : Normalising data  - à adapter à votre corpus \n",
    "- Spelling Correction\n",
    "- Replace Negations with Antonyms\n",
    "- Handling Capitalized Words\n",
    "- Lowercase\n",
    "- Tokenization\n",
    "- Remove Stopwords (ex: the, and….)\n",
    "- Stemming / racinisation (optional)\n",
    "- Lemmatizing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowball\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sentences \n\u001b[0;32m      5\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "stemmer = SnowballStemmer(language='french')\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "#    return \" \".join([token for token in text if token.lower() not in stop_words])\n",
    "    return [token for token in text if token.lower() not in stop_words_french]\n",
    "\n",
    "def stemming(text):\n",
    "    return [stemmer.stem(token) for token in text]\n",
    "\n",
    "def lemmatizing(text):\n",
    "    Text = \" \".join([token for token in text])\n",
    "    doc = nlp(Text)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizing(remove_stopwords(tokenize(Text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add stopwors to your list \n",
    "#stop_words_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_post(post):\n",
    "    \"\"\"Main master function to clean and normalizing posts, and tokenizing / Lemmatization\"\"\"\n",
    "    tweet = remove_links(post)\n",
    "    tweet = remove_tags(post)\n",
    "    tweet = remove_users(post)\n",
    "    tweet = remove_nonText(post)\n",
    " #   tweet = remove_contraction(tweet) # English only\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = tokenize(post)  # apply tokenization\n",
    "    tweet = remove_stopwords(post)\n",
    "#    tweet = stemming(tweet) # Optionnel \n",
    "    tweet = lemmatizing(tweet) # Optionnel\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer une liste regroupant tous les posts\n",
    "list_posts_clean = [ ]\n",
    "for post in list_posts:\n",
    "    list_posts_clean.append(tokenize(preprocess_post(post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_posts_clean[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "#Create a list of all words\n",
    "all_tokens_clean = sum(list_posts_clean, [])\n",
    "fdist_normalized = FreqDist(all_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Top 50 in all Macron posts : ---\" , fdist_normalized.most_common(50))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer une liste regroupant tous les textes par mois\n",
    "Collection_clean = [ ]\n",
    "\n",
    "for i in range(len(Collection)):\n",
    "    Collection_clean.append([tokenize(preprocess_post(text)) for text in Collection[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in Collection_clean[0][0:10]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the most frequent words\n",
    "for i in range(len(Collection_clean)):\n",
    "    all_tokens_clean_temp = sum(Collection_clean[i], [])\n",
    "    fdist_normalized_temp = FreqDist(all_tokens_clean_temp)\n",
    "    print( \"mois de\", list_period[i] ,\" : ---\" , )\n",
    "    print(fdist_normalized_temp.most_common(10))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche contextuelle : les n-gramms\n",
    "L’approche contextuelle : on s’intéresse non seulement aux mots et à leur fréquence, mais aussi aux mots qui suivent.\n",
    "Le calcul de n-grams (bigrams pour les co-occurences de mots deux-à-deux, tri-grams pour les co-occurences trois-à-trois, etc.) constitue la méthode la plus simple pour tenir compte du contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - Co-occurences\n",
    "Une cooccurrence est une combinaison de mots qui apparaissent fréquemment ensemble dans un corpus. Ainsi on retrouve dans les même textes les termes : « professeurs, étudiants », « médecin, infirmière », ou encore « pain, boulangerie »...\n",
    "Les collocations sont eux une forme privilégiée de cooccurrence : ce sont tout simplement des mots qui tendent à apparaître ensemble. Parmi les collocations en langues française, nous pouvons citer : « un désir ardent », « un léger frisson », ou encore « un regret amère ». "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_tokens_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mText(\u001b[43mall_tokens_clean\u001b[49m)\n\u001b[0;32m      2\u001b[0m text\u001b[38;5;241m.\u001b[39mconcordance(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretraites\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_tokens_clean' is not defined"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(all_tokens_clean)\n",
    "text.concordance(\"retraites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent collocations in the text (usually meaningful phrases)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Ngrams with the specific name \"guerre\"\n",
    "name_filter = lambda *w: 'guerre' not in w\n",
    "\n",
    "## Bigrams\n",
    "finder = nltk.BigramCollocationFinder.from_words(text)\n",
    "\n",
    "# only bigrams that contain 'guerre'\n",
    "finder.apply_ngram_filter(name_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest student ratio\n",
    "print(finder.nbest(bigram_measures.student_t,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest chi2 ratio\n",
    "print(finder.nbest(bigram_measures.chi_sq,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest likelihood_ratio\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the 10 bi-grams with the highest PMI ratio\n",
    "print(finder.nbest(bigram_measures.pmi,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer une liste pour afficher les résultats en tableau\n",
    "list_bigrams=[]\n",
    "list_bigrams.append(finder.nbest(bigram_measures.student_t,10))\n",
    "list_bigrams.append(finder.nbest(bigram_measures.chi_sq,10))\n",
    "list_bigrams.append(finder.nbest(bigram_measures.likelihood_ratio,10))\n",
    "list_bigrams.append(finder.nbest(bigram_measures.pmi,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_bigrams)\n",
    "df=df.transpose()\n",
    "df.columns = ['Student_t', 'Chi_sq', 'Likelihood_ratio','PMI']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Explore Co-occurring Words (Bigrams)\n",
    "Une cooccurrence est une combinaison de mots qui apparaissent fréquemment ensemble dans un corpus. Ainsi on retrouve dans les même textes les termes : « professeurs, étudiants », « médecin, infirmière », ou encore « pain, boulangerie »...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram = list(bigrams(all_tokens_clean))\n",
    "# View bigrams \n",
    "terms_bigram[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(terms_bigram)\n",
    "\n",
    "bigram_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(100),\n",
    "                             columns=['bigram', 'count'])\n",
    "\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c- Visualize Networks of Bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of bigrams and their counts\n",
    "d = bigram_df.set_index('bigram').T.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network plot \n",
    "G = nx.Graph()\n",
    "\n",
    "# Create connections between nodes\n",
    "for k, v in d[0].items():\n",
    "    G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "\n",
    "G.add_node(\"macron\", weight=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "pos = nx.spring_layout(G, k=2)\n",
    "\n",
    "# Plot networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=12,\n",
    "                 width=2,\n",
    "                 edge_color='grey',\n",
    "                 node_color='blue',\n",
    "                 node_size=50,\n",
    "                 with_labels = False,\n",
    "                 ax=ax)\n",
    "\n",
    "# Create offset labels\n",
    "for key, value in pos.items():\n",
    "    x, y = value[0]+.135, value[1]+.145\n",
    "    ax.text(x, y,\n",
    "            s=key,\n",
    "            bbox=dict(facecolor='lightblue', alpha=0.25),\n",
    "            horizontalalignment='center', fontsize=8)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - The Similarity Between Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a - Euclidean Distance\n",
    "Euclidean Distance is probably one of the most known formulas for computing the distance between two points applying the Pythagorean theorem. To get it you just need to subtract the points from the vectors, raise them to squares, add them up and take the square root of them. Did it seem complex? Don’t worry, in the image below it will be easier to understand.\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "df_similarity_euclidean = pd.DataFrame(euclidean_distances(vectors, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_euclidean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat = 2\n",
    "Similar_years=pd.DataFrame()\n",
    "print(\"We look for the most similar posts to: \")\n",
    "Similar_years ['Euclidean similarity'] = pd.DataFrame(df_similarity_euclidean[mandat])\n",
    "Similar_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similar_years.sort_values(by=['Euclidean similarity'] ,ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b - Manhattan Distance\n",
    "Although Manhattan distance seems to work okay for high-dimensional data, it is a measure that is somewhat less intuitive than euclidean distance, especially when using in high-dimensional data.\n",
    "\n",
    "When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "df_similarity_manhattan = pd.DataFrame(manhattan_distances(vectors, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_manhattan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat = 2\n",
    "print(\"We look for the most similar tweets to: \")\n",
    "Similar_years ['Manhattan similarity'] = pd.DataFrame(df_similarity_manhattan[mandat])\n",
    "Similar_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similar_years.sort_values(by=['Manhattan similarity'] ,ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c - Finding similar documents using cosine similarity\n",
    "Cosine Similarity is a method of calculating the similarity of two vectors by taking the dot product and dividing it by the magnitudes of each vector, as shown by the illustration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "url = \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*IhpY-6LYV75983THCpWo-w.png\"\n",
    "Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df_similarity_cosine = pd.DataFrame(cosine_similarity(vectors, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandat = 2\n",
    "print(\"We look for the most yers to: \" )\n",
    "Similar_years ['Cosine similarity'] = pd.DataFrame(df_similarity_cosine[mandat])\n",
    "Similar_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similar_years.sort_values(by=['Cosine similarity'] ,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes\n",
    "fig = plt.figure(figsize=(17,10), dpi=70)\n",
    "ax = fig.add_subplot(111)\n",
    "# Visualize the matrix with colored squares indicating similarity\n",
    "ax.matshow(df_similarity_cosine, cmap='rainbow_r', vmin = 0.0, vmax = 1)\n",
    "# Set regular ticks, one for each book in the collection\n",
    "ax.set_xticks(range(len(df_results[\"period\"])))\n",
    "ax.set_yticks(range(len(df_results[\"period\"])))\n",
    "# Set the tick labels as the book titles\n",
    "ax.set_xticklabels(df_results[\"period\"])\n",
    "ax.set_yticklabels(df_results[\"period\"])\n",
    "# Rotate the labels on the x-axis by 90 degrees\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
